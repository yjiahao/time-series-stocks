{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/processed/Mastercard_stock_history_processed.csv\")\n",
    "df.head()\n",
    "# df = df[[\"Open\", \"Volume\",  \"lag_1\",\"lag_2\",\"MA\",\"M_STD\", \"month\", \"day\",\"quarter\",\"Close\"]]\n",
    "df = df[[\"Volume\", \"Open\", \"day\", \"MA\", \"M_STD\", \"day\", \"quarter\", \"Close\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[:-100]\n",
    "test_df = df[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5482, 4, 8]),\n",
       " torch.Size([5482, 1]),\n",
       " torch.Size([96, 4, 8]),\n",
       " torch.Size([96, 1]))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dataset(dataset, n):\n",
    "    \"\"\"Transform a time series into a prediction dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: A numpy array of time series, first dimension is the time steps\n",
    "        n: Size of window for prediction\n",
    "    \"\"\"\n",
    "    X, y = [], [] \n",
    "    for i in range(len(dataset)-n):\n",
    "        feature = dataset[i:i+n, :] # (n, feature_size)\n",
    "        target = dataset[i+n, -1] # scalar (of next value)\n",
    "        X.append(feature)\n",
    "        y.append(target)\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# train_df_torch = torch.from_numpy(train_df.to_numpy()).type(torch.float32)\n",
    "# test_df_torch = torch.from_numpy(test_df.to_numpy()).type(torch.float32)\n",
    "X_train, y_train = create_dataset(train_df.to_numpy(), 4)\n",
    "X_test, y_test = create_dataset(test_df.to_numpy(), 4)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "# X_train, y_train = train_df_torch[:, :-1], train_df_torch[:, -1].reshape(-1, 1)\n",
    "# X_test, y_test= test_df_torch[:, :-1], test_df_torch[:, -1].reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # assume x is (N,L,input_size)\n",
    "        out, _ = self.lstm(x)\n",
    "        # print(\"out shape:\", out.shape)\n",
    "        return self.fc(out[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesANN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        # self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Define the fully connected layer\n",
    "        self.input = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.mlp_block1 = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ELU(alpha=0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LeakyReLU(negative_slope = 0.15),\n",
    "        )\n",
    "\n",
    "      \n",
    "        self.output_lyr = nn.Linear(512, output_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        a0 = self.input(x)\n",
    "        a1 = self.mlp_block1(a0)\n",
    "        a2 = self.output_lyr(a1)\n",
    "        return a2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if \"bias\" in name:\n",
    "                nn.init.zeros_(param)\n",
    "            elif \"weight\" in name:\n",
    "                nn.init.orthogonal_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train RMSE 124.6372, test RMSE 333.2244\n",
      "Epoch 1: train RMSE 119.7145, test RMSE 324.4249\n",
      "Epoch 2: train RMSE 115.8077, test RMSE 316.7710\n",
      "Epoch 3: train RMSE 112.6246, test RMSE 309.8889\n",
      "Epoch 4: train RMSE 110.0356, test RMSE 303.6576\n",
      "Epoch 5: train RMSE 107.7444, test RMSE 297.4245\n",
      "Epoch 6: train RMSE 106.1161, test RMSE 292.3638\n",
      "Epoch 7: train RMSE 104.5217, test RMSE 286.5466\n",
      "Epoch 8: train RMSE 103.6135, test RMSE 282.6058\n",
      "Epoch 9: train RMSE 102.9297, test RMSE 279.1304\n",
      "Epoch 10: train RMSE 102.4224, test RMSE 276.0694\n",
      "Epoch 11: train RMSE 102.0486, test RMSE 273.3635\n",
      "Epoch 12: train RMSE 101.6046, test RMSE 269.1047\n",
      "Epoch 13: train RMSE 101.3110, test RMSE 264.2186\n",
      "Epoch 14: train RMSE 94.8217, test RMSE 258.6304\n",
      "Epoch 15: train RMSE 87.6555, test RMSE 252.5172\n",
      "Epoch 16: train RMSE 88.7415, test RMSE 246.3384\n",
      "Epoch 17: train RMSE 97.0931, test RMSE 240.2852\n",
      "Epoch 18: train RMSE 94.7422, test RMSE 234.2229\n",
      "Epoch 19: train RMSE 85.7767, test RMSE 228.2627\n",
      "Epoch 20: train RMSE 81.4683, test RMSE 222.4270\n",
      "Epoch 21: train RMSE 79.6505, test RMSE 216.5845\n",
      "Epoch 22: train RMSE 76.5652, test RMSE 210.8508\n",
      "Epoch 23: train RMSE 68.5369, test RMSE 205.2334\n",
      "Epoch 24: train RMSE 71.4944, test RMSE 199.6432\n",
      "Epoch 25: train RMSE 66.9417, test RMSE 194.1499\n",
      "Epoch 26: train RMSE 64.0367, test RMSE 188.7684\n",
      "Epoch 27: train RMSE 59.9375, test RMSE 183.4155\n",
      "Epoch 28: train RMSE 72.8542, test RMSE 178.1854\n",
      "Epoch 29: train RMSE 59.6834, test RMSE 173.0191\n",
      "Epoch 30: train RMSE 56.2195, test RMSE 167.9310\n",
      "Epoch 31: train RMSE 53.0420, test RMSE 162.9805\n",
      "Epoch 32: train RMSE 51.6948, test RMSE 158.1088\n",
      "Epoch 33: train RMSE 52.3505, test RMSE 153.2988\n",
      "Epoch 34: train RMSE 57.3048, test RMSE 148.5607\n",
      "Epoch 35: train RMSE 58.6194, test RMSE 143.8558\n",
      "Epoch 36: train RMSE 56.6134, test RMSE 139.1777\n",
      "Epoch 37: train RMSE 42.7957, test RMSE 134.5593\n",
      "Epoch 38: train RMSE 51.3246, test RMSE 130.0377\n",
      "Epoch 39: train RMSE 47.1774, test RMSE 125.5814\n",
      "Epoch 40: train RMSE 34.7254, test RMSE 121.2306\n",
      "Epoch 41: train RMSE 38.1593, test RMSE 116.9797\n",
      "Epoch 42: train RMSE 36.3442, test RMSE 112.8155\n",
      "Epoch 43: train RMSE 55.9821, test RMSE 108.7049\n",
      "Epoch 44: train RMSE 38.8665, test RMSE 104.6909\n",
      "Epoch 45: train RMSE 44.5534, test RMSE 100.7733\n",
      "Epoch 46: train RMSE 39.6329, test RMSE 96.9693\n",
      "Epoch 47: train RMSE 34.0471, test RMSE 93.3130\n",
      "Epoch 48: train RMSE 28.6980, test RMSE 89.8073\n",
      "Epoch 49: train RMSE 31.9966, test RMSE 86.4437\n",
      "Epoch 50: train RMSE 32.9150, test RMSE 83.2238\n",
      "Epoch 51: train RMSE 36.7743, test RMSE 80.0079\n",
      "Epoch 52: train RMSE 28.1753, test RMSE 76.8687\n",
      "Epoch 53: train RMSE 29.1382, test RMSE 73.6169\n",
      "Epoch 54: train RMSE 28.7111, test RMSE 70.3621\n",
      "Epoch 55: train RMSE 43.8723, test RMSE 67.2298\n",
      "Epoch 56: train RMSE 34.8989, test RMSE 64.1959\n",
      "Epoch 57: train RMSE 37.1461, test RMSE 61.2673\n",
      "Epoch 58: train RMSE 43.3724, test RMSE 58.3371\n",
      "Epoch 59: train RMSE 36.6694, test RMSE 55.5911\n",
      "Epoch 60: train RMSE 33.4897, test RMSE 52.8807\n",
      "Epoch 61: train RMSE 35.0575, test RMSE 50.1820\n",
      "Epoch 62: train RMSE 45.1684, test RMSE 47.6503\n",
      "Epoch 63: train RMSE 45.5147, test RMSE 45.0824\n",
      "Epoch 64: train RMSE 93.6466, test RMSE 42.3229\n",
      "Epoch 65: train RMSE 139.0268, test RMSE 39.2503\n",
      "Epoch 66: train RMSE 125.5553, test RMSE 44.1690\n",
      "Epoch 67: train RMSE 162.6765, test RMSE 33.4725\n",
      "Epoch 68: train RMSE 242.7402, test RMSE 34.9222\n",
      "Epoch 69: train RMSE 121.0636, test RMSE 194.3889\n",
      "Epoch 70: train RMSE 159.4508, test RMSE 137.8484\n",
      "Epoch 71: train RMSE 147.1819, test RMSE 150.4068\n",
      "Epoch 72: train RMSE 144.2017, test RMSE 158.1098\n",
      "Epoch 73: train RMSE 133.0788, test RMSE 158.3465\n",
      "Epoch 74: train RMSE 89.1642, test RMSE 158.3089\n",
      "Epoch 75: train RMSE 251.6746, test RMSE 26.5977\n",
      "Epoch 76: train RMSE 134.8459, test RMSE 27.2148\n",
      "Epoch 77: train RMSE 86.0558, test RMSE 26.0639\n",
      "Epoch 78: train RMSE 141.1699, test RMSE 162.5200\n",
      "Epoch 79: train RMSE 142.8967, test RMSE 160.0877\n",
      "Epoch 80: train RMSE 176.0595, test RMSE 21.9094\n",
      "Epoch 81: train RMSE 223.4451, test RMSE 25.8259\n",
      "Epoch 82: train RMSE 126.9520, test RMSE 155.4827\n",
      "Epoch 83: train RMSE 153.7348, test RMSE 20.1496\n",
      "Epoch 84: train RMSE 136.6797, test RMSE 169.0030\n",
      "Epoch 85: train RMSE 121.0918, test RMSE 160.0521\n",
      "Epoch 86: train RMSE 85.1327, test RMSE 150.7290\n",
      "Epoch 87: train RMSE 86.9426, test RMSE 147.0931\n",
      "Epoch 88: train RMSE 84.1536, test RMSE 144.4802\n",
      "Epoch 89: train RMSE 73.2581, test RMSE 142.6191\n",
      "Epoch 90: train RMSE 84.1872, test RMSE 140.6352\n",
      "Epoch 91: train RMSE 202.2363, test RMSE 48.4252\n",
      "Epoch 92: train RMSE 144.8989, test RMSE 40.2201\n",
      "Epoch 93: train RMSE 122.5300, test RMSE 45.2718\n",
      "Epoch 94: train RMSE 123.6939, test RMSE 47.5777\n",
      "Epoch 95: train RMSE 117.8339, test RMSE 46.2143\n",
      "Epoch 96: train RMSE 104.6425, test RMSE 36.1559\n",
      "Epoch 97: train RMSE 97.1802, test RMSE 51.0410\n",
      "Epoch 98: train RMSE 86.3443, test RMSE 34.6385\n",
      "Epoch 99: train RMSE 93.9721, test RMSE 15.3931\n"
     ]
    }
   ],
   "source": [
    "model = TimeSeriesLSTM(\n",
    "    input_size=8,\n",
    "    hidden_size=40,\n",
    "    num_layers=5,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "# model.apply(initialize_weights)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[16, 50, 80], gamma=0.0001)\n",
    "# loader = load_data(\"data/processed/mastercard_stock_history_processed.csv\", batch_size=8)\n",
    "loader = DataLoader(TensorDataset(X_train, y_train), shuffle = False, batch_size = 8)\n",
    "train_rmse_lst = []\n",
    "test_rmse_lst = []\n",
    " \n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # scheduler.step()\n",
    "    # Validation\n",
    "    if epoch % 1 != 0:\n",
    "        continue\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_train)\n",
    "        train_rmse = np.sqrt(loss_fn(y_pred, y_train))\n",
    "        # train_rmse = loss_fn(y_pred, y_train)\n",
    "        y_pred = model(X_test)\n",
    "        test_rmse = np.sqrt(loss_fn(y_pred, y_test))\n",
    "\n",
    "        train_rmse_lst.append(train_rmse)\n",
    "        test_rmse_lst.append(test_rmse)\n",
    "        # test_rmse = loss_fn(y_pred, y_test)\n",
    "    print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f\" % (epoch, train_rmse, test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_rmse_lst, color = \"blue\")\n",
    "plt.plot(test_rmse_lst, color = \"orange\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To experiment with some deep learning models on the time series data given"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS2109S",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
